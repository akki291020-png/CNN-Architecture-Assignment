{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN Architecture | Assignment**"
      ],
      "metadata": {
        "id": "XAgKc9LLmcgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "sol)In a Convolutional Neural Network (CNN), filters and feature maps are the two most critical components for \"seeing\" and interpreting data.1 While the filter is the tool used to look for specific patterns, the feature map is the result or \"memory\" of what was found.21. Filters (Kernels or Feature Detectors)3A filter is a small matrix of weights (typically 3x3 or 5x5) that slides across the input image to perform a mathematical operation called convolution.4Role: Its primary job is to act as a pattern detector.5How it works: As the filter slides (strides) over the image, it performs element-wise multiplication with the pixel values and sums them up.6Learning: Initially, these filters are random.7 Through training (backpropagation), the network \"learns\" the best weight values to detect useful features.8Hierarchy:Early layers: Detect simple things like horizontal edges, vertical lines, or color blobs.9Deeper layers: Detect complex shapes like eyes, wheels, or textures.102. Feature Maps (Activation Maps)11A feature map is the output produced after a filter has finished scanning the entire input image.12 It is essentially a 2D grid that represents where a specific feature was found.13Role: Its job is to provide a spatial map of features.14Interpretation: High values in a feature map indicate that the filter \"found\" a strong match for its pattern at that specific location.15 For example, a \"vertical edge\" feature map will have bright pixels wherever a vertical edge exists in the original image.16Preservation: Unlike standard neural networks that flatten data, feature maps preserve the spatial relationship between pixels (e.g., they know the \"nose\" is above the \"mouth\").Summary ComparisonFeatureFilter (Kernel)Feature Map (Activation Map)What is it?A matrix of learnable weights.The output matrix (result).FunctionTo scan and detect patterns.To show the location of detected patterns.DimensionsSmall (e.g., 3x3x3).Varies based on input and stride.Analogy17A magnifying glass looking for a shape.18A highlighted map showing every spot that shape appeared.\n",
        "\n",
        "Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?\n",
        "sol)In a Convolutional Neural Network (CNN), Padding and Stride are essential hyperparameters that control the size of the resulting feature map and how the network extracts information from the input.1. PaddingPadding is the process of adding extra \"pixels\" (usually zeros) around the boundary of the input image before applying the filter.Why it's used:Prevents Shrinking: Without padding, the output dimensions shrink with every convolution layer, which limits the depth of the network [03:52].Edge Information: Pixels at the corners and edges of an image only participate in a few convolution operations compared to central pixels. Padding ensures that edge information is processed more effectively [01:26].Common Types:Valid Padding: No padding is added. The output size will always be smaller than the input [01:15].Same Padding: Padding is added such that the output feature map has the same dimensions as the input image [04:07].2. StrideStride refers to the number of pixels the filter shifts (slides) across the input image at each step [04:42].Stride of 1: The filter moves one pixel at a time, capturing the most detail and producing a larger feature map [04:54].Stride of 2 (or more): The filter \"jumps\" multiple pixels. This effectively downsamples the image, reducing the spatial dimensions of the output and computational cost [05:19].Impact on Output DimensionsThe size of the output feature map is determined by the input size, filter size, padding, and stride.HyperparameterChangeEffect on Output SizePaddingIncreaseIncreases the output size (or maintains it).StrideIncreaseDecreases the output size (the filter skips more pixels).Filter SizeIncreaseDecreases the output size (as the filter needs more \"room\" to fit).The Formula:If you want to calculate the exact output dimension ($O$):$$O = \\frac{(W - K + 2P)}{S} + 1$$$W$: Input width/height$K$: Filter (Kernel) size$P$: Padding$S$: Stride\n",
        "\n",
        "Question 3: Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?\n",
        "sol)In a Convolutional Neural Network (CNN), the Receptive Field (RF) is the specific region of the input image that a particular neuron \"sees\" or is influenced by.While a neuron in the first layer might only look at a tiny 3x3 patch of pixels, a neuron in the 10th layer might have a receptive field that covers almost the entire image.1. The Core ConceptThink of the receptive field as the \"field of view\" for a neuron.Local Receptive Field: In the first layer, this is simply the size of the filter (e.g., 3x3).Global Receptive Field: As you go deeper, each neuron's value is calculated from a patch of neurons in the previous layer. Since those neurons each looked at their own patches, the \"ancestral\" area in the original input image grows significantly.2. Why it is Critical for Deep ArchitecturesDeep architectures rely on the growth of the receptive field to transition from \"seeing\" pixels to \"understanding\" objects.Capturing Context: To recognize a \"face,\" a neuron must be able to see the relationship between the eyes, nose, and mouth. If the receptive field is too small (e.g., only 5x5 pixels), the neuron can only see a tiny bit of skin texture and will never \"know\" it is part of a face.Hierarchical Learning: * Small RF (Early Layers): Detects edges, dots, and simple colors.Large RF (Deep Layers): Detects complex shapes, objects, and spatial relationships.Effective Receptive Field (ERF): Interestingly, research shows that not all pixels in an RF are equal. The pixels in the center have a much higher impact on the output than those at the edges. In very deep networks, the \"Effective\" receptive field actually grows slower than the theoretical one.3. How to Increase Receptive FieldIf a task requires more global context (like understanding the layout of a room), architects use several tricks to boost the RF:Adding Layers: RF grows linearly with each added convolution.Sub-sampling (Pooling/Stride): RF grows multiplicatively, allowing the network to cover the whole image much faster.Dilated Convolons: This involves using filters with \"holes\" to scan a wider area without increasing the number of parameters.Summary TableAspectEarly LayersDeep LayersRF SizeSmall (Local)Large (Global)What it SeesLocal edges/texturesComplex objects/scenesInformationDetail-orientedContext-oriented\n",
        "\n",
        "Question 4: Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN.\n",
        "sol) In a CNN, the relationship between hyperparameters and parameter counts is often misunderstood. The most important distinction to make is that while both Filter Size and Stride affect the size of the output, only one of them actually changes the number of learnable parameters in the network.1. Filter Size (Kernel Size)Filter size has a direct and significant impact on the number of parameters.1Each filter consists of a grid of weights.2 If you increase the size of this grid, you are adding more weights that the model must learn during training.3Growth Rate: The number of parameters increases quadratically with the filter size. For example, moving from a 4$3 \\times 3$ filter to a 5$5 \\times 5$ filter doesn't just add a few weights—it increases the weights per filter from 6$9$ to 7$25$ (nearly 8$3 \\times$ more parameters).9The Formula: 10$$\\text{Parameters} = (\\text{Width} \\times \\text{Height} \\times \\text{Input Channels} + 1) \\times \\text{Number of Filters}$$(The 11$+1$ represents the bias term for each filter.)122. StrideStride has no direct impact on the number of parameters in the layer where it is applied.13Stride determines how many pixels the filter \"jumps\" as it scans the image.14 Whether a filter slides 1 pixel at a time or 10 pixels at a time, the filter itself (the matrix of weights) remains the same size.Fixed Weights: A 15$3 \\times 3$ filter with 16$64$ channels has the exact same number of parameters whether the stride is 17$1$, 18$2$, or 19$4$.20Indirect Influence: While it doesn't change the parameters of the current layer, it drastically reduces the output dimensions (the feature map).21 This indirectly reduces the number of parameters in subsequent Fully Connected (Dense) layers, because there are fewer total \"pixels\" to flatten and connect to the next set of neurons.Comparison TableFeatureChangeEffect on ParametersEffect on Output SizeFilter SizeIncrease (e.g., 3x3 $\\rightarrow$ 5x5)Increases (Quadratic)DecreasesStrideIncrease (e.g., 1 $\\rightarrow$ 2)No ChangeDecreases (Drastic)Why This Matters for DesignModern architectures like ResNet or VGG prefer stacking multiple small 22$3 \\times 3$ filters rather than using one large 23$7 \\times 7$ filter.24 This strategy achieves the same \"field of view\" (receptive field) while using significantly fewer parameters, making the model deeper and more efficient without becoming too \"heavy.\"\n",
        "\n",
        "Question 5: Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "sol)The evolution of CNNs from LeNet to AlexNet and VGG represents a journey from digit recognition to large-scale object classification, characterized primarily by increasing depth, more efficient activation functions, and more sophisticated filter strategies.Comparative OverviewFeatureLeNet-5 (1998)AlexNet (2012)VGG-16 (2014)Input Size$32 \\times 32$ (Grayscale)$227 \\times 227$ (RGB)$224 \\times 224$ (RGB)Total Layers7 (2 Conv, 2 Subsample, 3 FC)8 (5 Conv, 3 FC)16 (13 Conv, 3 FC)Filter Sizes$5 \\times 5$$11 \\times 11$, $5 \\times 5$, $3 \\times 3$Uniform $3 \\times 3$ActivationTanh / SigmoidReLU (First major use)ReLUPoolingAverage PoolingMax Pooling (Overlapping)Max Pooling ($2 \\times 2$)Parameters~60,000~60 Million~138 MillionPerformanceBasic Digit RecognitionBreakthrough (ILSVRC Winner)State-of-the-Art (at release)1. LeNet-5: The PioneerDesigned for handwritten digit recognition (MNIST), LeNet was the first to implement the classic sequence of Convolution 1$\\rightarrow$ Pooling 2$\\rightarrow$ Fully Connected layers.3Filter Strategy: Used relatively large $5 \\times 5$ filters for very small images.Limitations: Limited by 1990s hardware; it used Sigmoid/Tanh activations, which are prone to the \"vanishing gradient\" problem in deeper networks.42. AlexNet: The Deep Learning SparkAlexNet \"rekindled\" interest in neural networks by winning the 2012 ImageNet challenge.5 It was much deeper and wider than LeNet.Key Innovations: It introduced ReLU for faster training and Dropout to prevent overfitting.6Filter Sizes: Used a mix of very large filters ($11 \\times 11$) in early layers to cover a large field of view and smaller filters later.Hardware: It was the first major architecture to use GPUs for parallel processing.73. VGG-16: Simplicity in DepthVGG (Visual Geometry Group) revolutionized CNN design by showing that depth matters more than large filter sizes.8Uniformity: Instead of using 9$11 \\times 11$ filters, VGG uses only small 10$3 \\times 3$ filters stacked together.11Logic: Stacking two 12$3 \\times 3$ filters has the same receptive field as one 13$5 \\times 5$ filter but uses fewer parameters and introduces more non-linearities (via ReLU), helping the network learn more complex features.14Drawback: It is a very \"heavy\" model with over 138 million parameters, making it computationally expensive and slow to train.Summary of the EvolutionFrom Shallow to Deep: We moved from 7 layers (LeNet) to 16+ layers (VGG).From Large to Small Filters: Architectures moved away from large $11 \\times 11$ filters in favor of stacked $3 \\times 3$ filters to improve efficiency.Performance: Error rates dropped drastically—AlexNet famously reduced top-5 error by 10% compared to previous non-CNN methods, and VGG pushed that even further toward human-level performance.\n",
        "\n",
        "Question 6: Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation.\n"
      ],
      "metadata": {
        "id": "KUYamHc7mjn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# 1. Load and Preprocess the Data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Reshape data to include the channel dimension (28, 28, 1)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# 2. Build the Model Architecture\n",
        "model = models.Sequential([\n",
        "    # First Convolutional Layer: 32 filters, 3x3 kernel\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Second Convolutional Layer: 64 filters, 3x3 kernel\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Flattening for the Dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.2), # Dropout to prevent overfitting\n",
        "    layers.Dense(10, activation='softmax') # 10 output classes (digits 0-9)\n",
        "])\n",
        "\n",
        "# 3. Compile the Model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. Train the Model\n",
        "print(\"Starting training...\")\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# 5. Evaluate the Model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f'\\nTest Accuracy: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzU1B-Xdouzw",
        "outputId": "8601c2c6-2ca8-4c70-f9d0-362fb5e0f2ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 65ms/step - accuracy: 0.8392 - loss: 0.5099 - val_accuracy: 0.9828 - val_loss: 0.0543\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 63ms/step - accuracy: 0.9755 - loss: 0.0811 - val_accuracy: 0.9865 - val_loss: 0.0426\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 62ms/step - accuracy: 0.9818 - loss: 0.0585 - val_accuracy: 0.9898 - val_loss: 0.0344\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 73ms/step - accuracy: 0.9873 - loss: 0.0429 - val_accuracy: 0.9905 - val_loss: 0.0348\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 63ms/step - accuracy: 0.9897 - loss: 0.0334 - val_accuracy: 0.9885 - val_loss: 0.0389\n",
            "313/313 - 3s - 11ms/step - accuracy: 0.9889 - loss: 0.0343\n",
            "\n",
            "Test Accuracy: 98.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture.\n"
      ],
      "metadata": {
        "id": "8JDI3cI6oyEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, utils\n",
        "\n",
        "# Load CIFAR-10 data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values (0 to 1)\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels (10 classes)\n",
        "y_train = utils.to_categorical(y_train, 10)\n",
        "y_test = utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(f\"Input shape: {x_train.shape}\") # (50000, 32, 32, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL6FVkCxmmJV",
        "outputId": "e66ae068-f65d-4af5-af2e-32486428609b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 0us/step\n",
            "Input shape: (50000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. CNN Architecture"
      ],
      "metadata": {
        "id": "CrNrrc0vpxa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cifar10_model():\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Block 1\n",
        "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    # Block 2\n",
        "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    # Classification Head\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_cifar10_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "YdlVZajHpAaG",
        "outputId": "1affbb71-9d90-411a-e802-57322c6b7943"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,251,242\u001b[0m (4.77 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,251,242</span> (4.77 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,251,050\u001b[0m (4.77 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,251,050</span> (4.77 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation."
      ],
      "metadata": {
        "id": "m0oqYfUrpDgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Input: 1x28x28. Output: 16x24x24\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
        "        # Output: 32x8x8\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) # Pool 24x24 -> 12x12\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2)) # Pool 8x8 -> 4x4\n",
        "        x = x.view(-1, 32 * 4 * 4) # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "OFpfK9_SpH-W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Data Loaders and Setup"
      ],
      "metadata": {
        "id": "4U26wu7wp3OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations: Convert to tensor and normalize\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_loader = DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(datasets.MNIST('./data', train=False, transform=transform), batch_size=1000)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "E_mWpSicp6b0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Training Loop and Evaluation"
      ],
      "metadata": {
        "id": "e2_7UETwp83w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad(): # No gradient calculation for evaluation\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.2f}%)\\n')\n",
        "\n",
        "# Run Training\n",
        "for epoch in range(1, 3):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig74cSerp_WZ",
        "outputId": "e6721c7d-c1e4-4269-81ae-1116166cb1e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000] Loss: 2.295141\n",
            "Train Epoch: 1 [6400/60000] Loss: 0.123726\n",
            "Train Epoch: 1 [12800/60000] Loss: 0.118151\n",
            "Train Epoch: 1 [19200/60000] Loss: 0.141145\n",
            "Train Epoch: 1 [25600/60000] Loss: 0.031585\n",
            "Train Epoch: 1 [32000/60000] Loss: 0.084298\n",
            "Train Epoch: 1 [38400/60000] Loss: 0.026706\n",
            "Train Epoch: 1 [44800/60000] Loss: 0.068865\n",
            "Train Epoch: 1 [51200/60000] Loss: 0.010211\n",
            "Train Epoch: 1 [57600/60000] Loss: 0.120313\n",
            "\n",
            "Test set: Average loss: 0.0510, Accuracy: 9840/10000 (98.40%)\n",
            "\n",
            "Train Epoch: 2 [0/60000] Loss: 0.026989\n",
            "Train Epoch: 2 [6400/60000] Loss: 0.125157\n",
            "Train Epoch: 2 [12800/60000] Loss: 0.061728\n",
            "Train Epoch: 2 [19200/60000] Loss: 0.090296\n",
            "Train Epoch: 2 [25600/60000] Loss: 0.079454\n",
            "Train Epoch: 2 [32000/60000] Loss: 0.159686\n",
            "Train Epoch: 2 [38400/60000] Loss: 0.004755\n",
            "Train Epoch: 2 [44800/60000] Loss: 0.117958\n",
            "Train Epoch: 2 [51200/60000] Loss: 0.020991\n",
            "Train Epoch: 2 [57600/60000] Loss: 0.030315\n",
            "\n",
            "Test set: Average loss: 0.0458, Accuracy: 9847/10000 (98.47%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model."
      ],
      "metadata": {
        "id": "lw5wuSDVqB2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset/\n",
        "    train/\n",
        "        cat/ (images...)\n",
        "        dog/ (images...)\n",
        "    validation/\n",
        "        cat/ (images...)\n",
        "        dog/ (images...)"
      ],
      "metadata": {
        "id": "qRyK-kEzrpbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Preprocessing with ImageDataGenerator"
      ],
      "metadata": {
        "id": "dFz03OCqqdt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# 1. Define Generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,           # Normalize pixels to [0, 1]\n",
        "    rotation_range=20,        # Randomly rotate images\n",
        "    width_shift_range=0.2,    # Randomly shift horizontally\n",
        "    height_shift_range=0.2,   # Randomly shift vertically\n",
        "    horizontal_flip=True,     # Randomly flip images\n",
        "    zoom_range=0.2            # Randomly zoom in/out\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255) # Only rescale for validation\n",
        "\n",
        "# 2. Load Images from Directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'dataset/train',\n",
        "    target_size=(150, 150),   # Resize all images to 150x150\n",
        "    batch_size=32,\n",
        "    class_mode='binary'       # Use 'categorical' if more than 2 classes\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    'dataset/validation',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "id": "LmCDO_o_rtys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Building and Training the CNN"
      ],
      "metadata": {
        "id": "0BlfGXefqmrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define the CNN Architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid') # Use 'softmax' with N neurons for multiclass\n",
        "])\n",
        "\n",
        "# 4. Compile\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 5. Train using the generator\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // 32,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // 32\n",
        ")"
      ],
      "metadata": {
        "id": "dBi05GwmryxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal”\n",
        "and “Pneumonia” categories. Describe your end-to-end approach–from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit\n",
        "sol) Building a medical imaging application requires a rigorous end-to-end pipeline, as model reliability and deployment accessibility are just as important as the architecture itself.Here is the strategic approach to building and deploying this classifier.1. Data Preparation & PreprocessingMedical images like X-rays (often in DICOM or high-res JPEG format) require specific handling.Class Imbalance: Pneumonia datasets often have more \"Pneumonia\" cases than \"Normal.\" I would use oversampling or class weights in the loss function to ensure the model doesn't become biased.Normalization: X-rays have varying contrast levels. I'd apply Histogram Equalization to standardize brightness across the dataset.Data Augmentation: To make the model robust, I'd apply subtle rotations, horizontal flips, and zooms. Note: Vertical flips are avoided as they don't represent realistic X-ray orientations.2. Model Architecture: Transfer LearningGiven that medical datasets are often small, training from scratch is inefficient. I would use Transfer Learning with a pre-trained DenseNet121 or ResNet50 model.Why DenseNet? DenseNet is widely used in medical imaging because its \"dense blocks\" allow for better feature propagation, which is crucial for identifying subtle patterns like opacities in lungs.Fine-tuning: I would freeze the initial layers (which detect basic edges) and train the final custom \"Classification Head\" on the X-ray data.3. Training and EvaluationLoss Function: BinaryCrossentropy since this is a two-class problem.Metrics: Accuracy is misleading in medicine. I would prioritize Recall (Sensitivity)—ensuring we don't miss any pneumonia cases (false negatives)—and AUC-ROC.Callbacks: Use EarlyStopping to prevent overfitting and ReduceLROnPlateau to fine-tune the learning rate as the loss plateaus.4. Deployment with StreamlitOnce the model is trained, I would save it as a .h5 or SavedModel format and build a user-friendly interface using Streamlit.The Streamlit Workflow:Image Upload: Use st.file_uploader to let doctors upload an X-ray.Processing: The app resizes the image to the model's required input size (e.g., $224 \\times 224$).Inference: The model runs a prediction and returns a probability.Results: Display the result with a confidence score.Example Code Snippet:\n"
      ],
      "metadata": {
        "id": "w5XtvFVZqs8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "st.title(\"Chest X-Ray Pneumonia Detector\")\n",
        "model = tf.keras.models.load_model('pneumonia_model.h5')\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload a Chest X-ray...\", type=\"jpg\")\n",
        "\n",
        "if uploaded_file:\n",
        "    image = Image.open(uploaded_file).convert('RGB')\n",
        "    st.image(image, caption='Uploaded X-ray', use_column_width=True)\n",
        "\n",
        "    # Preprocess\n",
        "    img = image.resize((224, 224))\n",
        "    img_array = np.expand_dims(np.array(img) / 255.0, axis=0)\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(img_array)\n",
        "    label = \"Pneumonia\" if prediction[0][0] > 0.5 else \"Normal\"\n",
        "    confidence = prediction[0][0] if label == \"Pneumonia\" else 1 - prediction[0][0]\n",
        "\n",
        "    st.write(f\"### Prediction: {label}\")\n",
        "    st.write(f\"Confidence: {confidence*100:.2f}%\")"
      ],
      "metadata": {
        "id": "N3XxhwA5r-2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Deployment Strategy\n",
        "To make the app accessible:\n",
        "\n",
        "Containerization: Wrap the app in Docker to ensure it runs the same way on any server.\n",
        "\n",
        "Cloud Hosting: Deploy the Docker container to Google Cloud Run or AWS App Runner.\n",
        "\n",
        "API Layer (Optional): If the startup scales, I would move the model to FastAPI to handle requests and let the Streamlit app act as a thin client."
      ],
      "metadata": {
        "id": "-JZ7F6_1rJ4H"
      }
    }
  ]
}